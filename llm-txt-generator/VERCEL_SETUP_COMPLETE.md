# ‚úÖ Vercel Functions Setup Complete

Your Python backend has been successfully converted to Vercel serverless functions! Here's what was created and how to deploy.

## üìÅ What Was Added

### 1. Vercel Functions (`/api/` directory)
- **`api/generate.py`** - Main endpoint that handles website crawling and llms.txt generation
- **`api/health.py`** - Health check endpoint

### 2. Configuration Files
- **`requirements.txt`** - Python dependencies (beautifulsoup4, aiohttp)
- **`vercel.json`** - Vercel configuration for Python functions and routing

### 3. Updated Frontend
- **`src/app/page.tsx`** - Updated to use environment variables for API URL
- Now supports both local development and Vercel deployment

### 4. Deployment Tools
- **`deploy-vercel.sh`** - Automated deployment script
- **`VERCEL_DEPLOYMENT.md`** - Comprehensive deployment guide

## üöÄ How to Deploy

### Option 1: Quick Deploy (Recommended)
```bash
./deploy-vercel.sh
```

### Option 2: Manual Deploy
```bash
# Build frontend
npm run build

# Deploy to Vercel
vercel --prod
```

### Option 3: GitHub Integration
1. Push your code to GitHub
2. Connect repository to Vercel
3. Vercel will auto-deploy

## üîß Environment Configuration

After deployment, set this environment variable in your Vercel dashboard:

```
NEXT_PUBLIC_API_URL = https://your-app.vercel.app
```

Or leave it empty to use relative URLs (recommended):
```
NEXT_PUBLIC_API_URL = 
```

## üß™ Testing Your Deployment

### 1. Health Check
```bash
curl https://your-app.vercel.app/api/health
```

Expected response:
```json
{
  "status": "healthy",
  "message": "LLMs.txt Generator API is running",
  "version": "1.0.0"
}
```

### 2. Generate llms.txt
```bash
curl -X POST "https://your-app.vercel.app/api/generate" \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://fastapi.tiangolo.com",
    "max_pages": 10,
    "depth_limit": 2
  }'
```

### 3. Frontend Test
Visit `https://your-app.vercel.app` and try generating an llms.txt file.

## üîÑ Key Differences from FastAPI

| FastAPI Backend | Vercel Functions |
|----------------|------------------|
| `uvicorn main:app` | Serverless functions |
| `@app.post("/generate")` | `api/generate.py` |
| `@app.get("/health")` | `api/health.py` |
| Always running | Cold start delays |
| Single server | Auto-scaling |

## ‚ö° Performance Considerations

### Vercel Function Limits
- **Execution Time**: 10s (Hobby), 60s (Pro)
- **Memory**: 1024MB max
- **Payload**: 4.5MB limit
- **Cold Starts**: ~1-3 seconds

### Optimizations Made
- Reduced dependencies to essentials only
- Shorter timeouts to prevent function timeouts
- Proper error handling for debugging
- CORS headers for frontend compatibility

## üêõ Troubleshooting

### Common Issues

1. **Function Timeout**
   - Reduce `max_pages` in requests (try 10-15)
   - Use faster websites for testing

2. **Cold Start Delays**
   - First request may be slow (normal)
   - Subsequent requests will be faster

3. **CORS Issues**
   - Check `NEXT_PUBLIC_API_URL` configuration
   - Verify CORS headers in function responses

### Debug Commands
```bash
# Check Vercel logs
vercel logs

# Test locally
vercel dev

# Check function status
curl https://your-app.vercel.app/api/health
```

## üìä Cost Estimation

### Vercel Pricing (Hobby Plan - Free)
- **100GB-hours** of function execution per month
- **100GB** bandwidth per month
- **Unlimited** static requests

### Typical Usage
- Health check: ~50ms execution
- Generate request: ~5-15 seconds execution
- Small site (10 pages): ~5-8 seconds
- Medium site (20 pages): ~10-15 seconds

## üéØ Next Steps

1. **Deploy**: Run `./deploy-vercel.sh`
2. **Test**: Verify both endpoints work
3. **Configure**: Set environment variables
4. **Monitor**: Check Vercel dashboard for metrics
5. **Optimize**: Adjust based on usage patterns

## üìö Additional Resources

- [Vercel Functions Documentation](https://vercel.com/docs/functions)
- [Python on Vercel](https://vercel.com/docs/functions/serverless-functions/runtimes/python)
- [VERCEL_DEPLOYMENT.md](./VERCEL_DEPLOYMENT.md) - Detailed deployment guide

---

üéâ **Ready to deploy!** Your LLMs.txt Generator is now configured for Vercel serverless functions. 